import os
import re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict
import json
import numpy as np
from scipy import stats

# ================= é…ç½®åŒºåŸŸ =================
# æ•°æ®æ ¹ç›®å½•
ROOT_DIR = "/data2/shaos/labs/mllms_know_94.1_head_select/head_analysisi/"
# è¦åˆ†æçš„ç›®æ ‡æ–‡ä»¶å
TARGET_FILENAME = "gt_region_head_analysis.txt"
# å›¾è¡¨ä¿å­˜ç›®å½•
OUTPUT_DIR = "/data2/shaos/labs/mllms_know_94.1_head_select/head_analysisi/head_analysis_results"
# æ³¨æ„åŠ›è´¡çŒ®å‰ n çš„æ•°é‡åˆ—è¡¨ - ä¿å­˜å¤šä¸ªä¸åŒnå€¼çš„ç»“æœ
TOP_N_VALUES = list(range(1, 101))
# æŒ‡å®šè¦åˆ†æçš„å±‚èŒƒå›´ (ä»ç¬¬ Lx å±‚åˆ°ç¬¬ Ly å±‚ï¼ŒLx <= Ly)
LAYER_RANGE = (0, 27)  # ä¾‹å¦‚ä» L1 åˆ° L5
# ===========================================

def parse_file_with_attention_ratio(file_path: str) -> List[Dict]:
    """
    è§£ææ–‡ä»¶ï¼Œæå–æ³¨æ„åŠ›å æ¯”ç›¸å…³çš„æ•°æ®
    ä¼˜å…ˆæŒ‰æ³¨æ„åŠ›å æ¯”(attention_ratio)æ’åº
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return []

    heads_data = []
    
    # æ–¹æ³•1: å°è¯•ä»CSVæ–‡ä»¶è¯»å–ï¼ˆæ›´å¯é ï¼‰
    csv_file = file_path.replace(".txt", ".csv")
    if os.path.exists(csv_file):
        try:
            csv_df = pd.read_csv(csv_file)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦çš„åˆ—
            required_columns = ['layer', 'head', 'attention_ratio']
            if all(col in csv_df.columns for col in required_columns):
                for _, row in csv_df.iterrows():
                    record = {
                        'Layer': int(row['layer']),
                        'Head': int(row['head']),
                        'AttentionRatio': float(row['attention_ratio'])
                    }
                    
                    # æ·»åŠ å…¶ä»–å¯é€‰å­—æ®µ
                    if 'mean_attention' in csv_df.columns:
                        record['MeanAttention'] = float(row['mean_attention'])
                    if 'relative_mean' in csv_df.columns:
                        record['RelativeContribution'] = float(row['relative_mean'])
                    if 'contrast_ratio' in csv_df.columns:
                        record['Contrast'] = float(row['contrast_ratio'])
                    if 'covered_patches' in csv_df.columns:
                        record['CoveredPatches'] = int(row['covered_patches'])
                    if 'total_attention' in csv_df.columns:
                        record['TotalAttention'] = float(row['total_attention'])
                    
                    heads_data.append(record)
                
                print(f"âœ… ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®: {csv_file} ({len(heads_data)} æ¡è®°å½•)")
                return heads_data
        except Exception as e:
            print(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {csv_file}, é”™è¯¯: {e}")

    # æ–¹æ³•2: å¦‚æœCSVæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œå°è¯•è§£æTXTæ–‡ä»¶
    print(f"å°è¯•è§£æTXTæ–‡ä»¶: {file_path}")
    
    # å°è¯•å¤šç§è¡¨æ ¼æ ¼å¼
    patterns_to_try = [
        # æ ¼å¼1: Top 50 Heads by Attention Ratio
        (r"Top \d+ Heads by Attention Ratio[\s\S]*?Rank\s+Layer\s+Head\s+AttRatio\s+MeanAtt\s+TotalAtt\s+GlobalTotal\s+Contrast\s+Patches[\s\S]*?(\d+\s+\d+\s+\d+\s+[\d\.]+\s+[\d\.]+\s+[\d\.]+\s+[\d\.]+\s+[\d\.]+\s+\d+)"),
        
        # æ ¼å¼2: Top 30 Heads by Mean Attention (åŒ…å«AttRatio)
        (r"Top \d+ Heads by Mean Attention[\s\S]*?Rank\s+Layer\s+Head\s+MeanAtt\s+RelMean\s+AttRatio\s+Contrast\s+Patches[\s\S]*?(\d+\s+\d+\s+\d+\s+[\d\.]+\s+[\d\.]+\s+[\d\.]+\s+[\d\.]+\s+\d+)"),
        
        # æ ¼å¼3: é€šç”¨è¡¨æ ¼è¡ŒåŒ¹é…
        (r"^\s*(\d+)\s+(\d+)\s+(\d+)\s+([0-9\.]+)\s+([0-9\.]+)\s+([0-9\.]+)\s+([0-9\.]+)\s+([0-9\.]+)\s+(\d+)", re.MULTILINE)
    ]
    
    for pattern_idx, pattern in enumerate(patterns_to_try):
        try:
            if pattern_idx < 2:  # å‰ä¸¤ä¸ªæ¨¡å¼æ˜¯æ‰¾ç‰¹å®šéƒ¨åˆ†
                if pattern in content:
                    section_match = re.search(pattern, content)
                    if section_match:
                        # æå–è¡¨æ ¼æ•°æ®éƒ¨åˆ†
                        table_section = section_match.group(1)
                        # åœ¨è¡¨æ ¼éƒ¨åˆ†ä¸­åŒ¹é…è¡Œ
                        row_pattern = r"^\s*\d+\s+\d+\s+\d+\s+([0-9\.]+)\s+([0-9\.]+)\s+([0-9\.]+)\s+([0-9\.]+)\s+([0-9\.]+)\s+(\d+)"
                        row_matches = re.findall(row_pattern, table_section, re.MULTILINE)
                        
                        for match in row_matches:
                            try:
                                record = {
                                    'Layer': int(match[0]),  # éœ€è¦æ ¹æ®å®é™…æ ¼å¼è°ƒæ•´ç´¢å¼•
                                    'Head': int(match[1]),
                                    'AttentionRatio': float(match[2]),
                                    'MeanAttention': float(match[3]),
                                    'TotalAttention': float(match[4]),
                                    'Contrast': float(match[5]),
                                    'CoveredPatches': int(match[6])
                                }
                                heads_data.append(record)
                            except Exception as e:
                                continue
                        
                        if heads_data:
                            print(f"âœ… ä»TXTæ–‡ä»¶è§£ææ•°æ®æˆåŠŸ (æ¨¡å¼{pattern_idx+1}): {len(heads_data)} æ¡è®°å½•")
                            return heads_data
            else:  # ç¬¬ä¸‰ä¸ªæ¨¡å¼æ˜¯é€šç”¨åŒ¹é…
                matches = re.findall(pattern, content, re.MULTILINE)
                for match in matches:
                    try:
                        # è·³è¿‡è¡¨å¤´è¡Œ
                        if match[0].isdigit() and int(match[0]) > 0:
                            record = {
                                'Layer': int(match[1]),
                                'Head': int(match[2]),
                                'AttentionRatio': float(match[3]),
                                'MeanAttention': float(match[4]),
                                'TotalAttention': float(match[5]),
                                'GlobalTotal': float(match[6]),
                                'Contrast': float(match[7]),
                                'CoveredPatches': int(match[8])
                            }
                            heads_data.append(record)
                    except Exception as e:
                        continue
                
                if heads_data:
                    print(f"âœ… ä»TXTæ–‡ä»¶è§£ææ•°æ®æˆåŠŸ (é€šç”¨æ¨¡å¼): {len(heads_data)} æ¡è®°å½•")
                    return heads_data
        except Exception as e:
            continue
    
    # å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ç®€å•çš„è¡ŒåŒ¹é…
    print(f"âš ï¸  ä½¿ç”¨ç®€å•æ¨¡å¼è§£æTXTæ–‡ä»¶: {file_path}")
    simple_pattern = r"(\d+)\s+(\d+)\s+(\d+)\s+([0-9\.]+)"
    matches = re.findall(simple_pattern, content)
    
    for match in matches:
        try:
            # ç¡®ä¿æ˜¯æ•°æ®è¡Œè€Œä¸æ˜¯è¡¨å¤´
            if match[0].isdigit() and int(match[0]) > 0:
                record = {
                    'Layer': int(match[1]),
                    'Head': int(match[2]),
                    'AttentionRatio': float(match[3])
                }
                heads_data.append(record)
        except Exception as e:
            continue
    
    if heads_data:
        print(f"âœ… ä»TXTæ–‡ä»¶è§£ææ•°æ®æˆåŠŸ (ç®€å•æ¨¡å¼): {len(heads_data)} æ¡è®°å½•")
    else:
        print(f"âŒ æ— æ³•ä»æ–‡ä»¶ä¸­è§£ææ•°æ®: {file_path}")
    
    return heads_data

def save_top_n_heads_to_jsonl(ranked_df: pd.DataFrame, title: str, n_values: List[int], layer_range: tuple):
    """
    å°†æŒ‡å®šå±‚èŒƒå›´å†…çš„å¤šä¸ªä¸åŒnå€¼çš„å‰nä¸ªå¤´çš„è´¡çŒ®æ•°æ®ä¿å­˜åˆ°jsonlæ–‡ä»¶ä¸­
    æŒ‰æ³¨æ„åŠ›å æ¯”æ’åº
    
    å‚æ•°:
        ranked_df: æ’åºåçš„DataFrame
        title: æ–‡ä»¶æ ‡é¢˜
        n_values: è¦ä¿å­˜çš„nå€¼åˆ—è¡¨
        layer_range: å±‚èŒƒå›´
    """
    # ç­›é€‰æŒ‡å®šå±‚èŒƒå›´çš„æ•°æ®
    filtered_df = ranked_df[(ranked_df['Layer'] >= layer_range[0]) & 
                           (ranked_df['Layer'] <= layer_range[1])].copy()
    
    if filtered_df.empty:
        print(f"âš ï¸  åœ¨å±‚èŒƒå›´ L{layer_range[0]}-L{layer_range[1]} å†…æ²¡æœ‰æ•°æ®")
        return []
    
    # æŒ‰æ³¨æ„åŠ›å æ¯”é™åºæ’åº
    filtered_df = filtered_df.sort_values(by='AttRatio_mean', ascending=False).reset_index(drop=True)
    
    saved_files = []
    
    for n in n_values:
        # é™åˆ¶nä¸è¶…è¿‡æ•°æ®é•¿åº¦
        actual_n = min(n, len(filtered_df))
        
        # å–å‰actual_nä¸ª
        top_n_df = filtered_df.head(actual_n)
        
        # ä¿å­˜åˆ°JSONLæ–‡ä»¶
        jsonl_file = os.path.join(OUTPUT_DIR, f"{title}_top_{actual_n}_heads_by_ratio_L{layer_range[0]}_to_L{layer_range[1]}.jsonl")
        
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for _, row in top_n_df.iterrows():
                # åˆ›å»ºä¿å­˜çš„è®°å½•
                record = {
                    'Layer': int(row['Layer']),
                    'Head': int(row['Head']),
                    'AttentionRatio': float(row['AttRatio_mean']),
                    'HeadLabel': f"L{int(row['Layer'])}-H{int(row['Head'])}",
                    'Rank': int(_ + 1)  # æ·»åŠ æ’åä¿¡æ¯
                }
                
                # æ·»åŠ å…¶ä»–å¯ç”¨å­—æ®µ
                if 'MeanAttention' in row:
                    record['MeanAttention'] = float(row['MeanAttention'])
                if 'RelativeContribution' in row:
                    record['RelativeContribution'] = float(row['RelativeContribution'])
                if 'TotalAttention' in row:
                    record['TotalAttention'] = float(row['TotalAttention'])
                if 'Contrast' in row:
                    record['Contrast'] = float(row['Contrast'])
                if 'CoveredPatches' in row:
                    record['CoveredPatches'] = int(row['CoveredPatches'])
                if 'Frequency' in row:
                    record['Frequency'] = int(row['Frequency'])
                
                json.dump(record, f, ensure_ascii=False)
                f.write("\n")
        
        print(f"âœ… å‰ {actual_n} ä¸ªæ³¨æ„åŠ›å¤´ï¼ˆæŒ‰æ³¨æ„åŠ›å æ¯”æ’åºï¼Œå±‚èŒƒå›´: L{layer_range[0]} åˆ° L{layer_range[1]}ï¼‰å·²ä¿å­˜è‡³: {jsonl_file}")
        
        # åŒæ—¶ä¿å­˜ä¸ºCSVæ ¼å¼ä»¥ä¾¿æŸ¥çœ‹
        csv_file = jsonl_file.replace('.jsonl', '.csv')
        top_n_df.to_csv(csv_file, index=False)
        print(f"âœ… CSVæ ¼å¼å·²ä¿å­˜è‡³: {csv_file}")
        
        saved_files.append({
            'n': actual_n,
            'jsonl_file': jsonl_file,
            'csv_file': csv_file,
            'num_heads': len(top_n_df)
        })
    
    # ç”Ÿæˆä¸€ä¸ªæ±‡æ€»æ–‡ä»¶ï¼Œè®°å½•æ‰€æœ‰nå€¼çš„ç»“æœ
    summary_file = os.path.join(OUTPUT_DIR, f"{title}_top_n_summary_L{layer_range[0]}_to_L{layer_range[1]}.txt")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"Top-N Heads Summary for {title}\n")
        f.write(f"Layer Range: L{layer_range[0]} to L{layer_range[1]}\n")
        f.write(f"Total available heads: {len(filtered_df)}\n")
        f.write("="*60 + "\n\n")
        
        for saved in saved_files:
            f.write(f"Top {saved['n']} heads:\n")
            f.write(f"  JSONL file: {os.path.basename(saved['jsonl_file'])}\n")
            f.write(f"  CSV file: {os.path.basename(saved['csv_file'])}\n")
            f.write(f"  Number of heads saved: {saved['num_heads']}\n")
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            if saved['n'] <= len(filtered_df):
                top_n_data = filtered_df.head(saved['n'])
                total_ratio = top_n_data['AttRatio_mean'].sum()
                percentage = (total_ratio / filtered_df['AttRatio_mean'].sum() * 100) if filtered_df['AttRatio_mean'].sum() > 0 else 0
                f.write(f"  Total attention ratio: {total_ratio:.6f}\n")
                f.write(f"  Percentage of total ratio: {percentage:.2f}%\n")
            
            f.write("\n")
    
    print(f"âœ… Top-Næ±‡æ€»æ–‡ä»¶å·²ä¿å­˜è‡³: {summary_file}")
    
    return saved_files

def generate_report_and_plots(df: pd.DataFrame, title: str):
    """
    ç”Ÿæˆæ’è¡Œæ¦œæ–‡æœ¬å’Œå¯è§†åŒ–å›¾è¡¨
    æŒ‰æ³¨æ„åŠ›å æ¯”æ’åº
    """
    if df.empty:
        print(f"No data for {title}")
        return
    
    print(f"\n{'='*60}")
    print(f"åˆ†ææ•°æ®é›†: {title}")
    print(f"æ€»æ•°æ®ç‚¹æ•°: {len(df)}")
    print(f"å”¯ä¸€å¤´æ•°: {df[['Layer', 'Head']].drop_duplicates().shape[0]}")
    
    if 'AttentionRatio' not in df.columns:
        print("âŒ æ•°æ®ä¸­æ²¡æœ‰AttentionRatioåˆ—")
        return
    
    print(f"æ³¨æ„åŠ›å æ¯”èŒƒå›´: {df['AttentionRatio'].min():.6f} åˆ° {df['AttentionRatio'].max():.6f}")
    print(f"å¹³å‡æ³¨æ„åŠ›å æ¯”: {df['AttentionRatio'].mean():.6f}")
    print(f"{'='*60}")

    # 1. æ•°æ®èšåˆ - æŒ‰(Layer, Head)åˆ†ç»„
    # å…ˆæ”¶é›†æ‰€æœ‰å¯èƒ½çš„åˆ†ç»„åˆ—
    agg_dict = {
        'AttentionRatio': ['mean', 'max', 'min', 'std', 'count']
    }
    
    # åŠ¨æ€æ·»åŠ å…¶ä»–åˆ—
    additional_columns = []
    if 'MeanAttention' in df.columns:
        agg_dict['MeanAttention'] = 'mean'
        additional_columns.append('MeanAttention_mean')
    if 'RelativeContribution' in df.columns:
        agg_dict['RelativeContribution'] = 'mean'
        additional_columns.append('RelativeContribution_mean')
    if 'Contrast' in df.columns:
        agg_dict['Contrast'] = 'mean'
        additional_columns.append('Contrast_mean')
    if 'CoveredPatches' in df.columns:
        agg_dict['CoveredPatches'] = 'mean'
        additional_columns.append('CoveredPatches_mean')
    if 'TotalAttention' in df.columns:
        agg_dict['TotalAttention'] = 'mean'
        additional_columns.append('TotalAttention_mean')
    
    # æ‰§è¡Œåˆ†ç»„èšåˆ
    agg_df = df.groupby(['Layer', 'Head']).agg(agg_dict).reset_index()
    
    # æ‰å¹³åŒ–åˆ—å
    agg_df.columns = ['Layer', 'Head'] + [f'AttRatio_{stat}' for stat in ['mean', 'max', 'min', 'std', 'count']] + additional_columns
    
    # é‡å‘½åcountåˆ—ä¸ºFrequency
    agg_df = agg_df.rename(columns={'AttRatio_count': 'Frequency'})
    
    # æŒ‰å¹³å‡æ³¨æ„åŠ›å æ¯”é™åºæ’åº
    ranked_df = agg_df.sort_values(by='AttRatio_mean', ascending=False).reset_index(drop=True)
    
    # åˆ›å»º Head çš„æ ‡ç­¾åˆ—
    ranked_df['HeadLabel'] = ranked_df.apply(lambda x: f"L{int(x['Layer'])}-H{int(x['Head'])}", axis=1)

    # --- æ‰“å°æ’è¡Œæ¦œ ---
    print(f"\n{'='*20} ğŸ† æ³¨æ„åŠ›å æ¯”æ’è¡Œæ¦œ: {title} {'='*20}")
    print(f"æŒ‰å¹³å‡æ³¨æ„åŠ›å æ¯”(AttRatio_mean)æ’åº")
    
    # åˆ›å»ºæ˜¾ç¤ºåˆ—
    display_columns = ['HeadLabel', 'AttRatio_mean', 'AttRatio_max', 'AttRatio_std', 'Frequency']
    
    # åŠ¨æ€æ·»åŠ å…¶ä»–ç»Ÿè®¡åˆ—
    if 'MeanAttention_mean' in ranked_df.columns:
        display_columns.append('MeanAttention_mean')
    if 'RelativeContribution_mean' in ranked_df.columns:
        display_columns.append('RelativeContribution_mean')
    if 'Contrast_mean' in ranked_df.columns:
        display_columns.append('Contrast_mean')
    
    # åˆ›å»ºæ ¼å¼åŒ–å‡½æ•°
    def format_float(x):
        if isinstance(x, (int, np.integer)):
            return f"{x}"
        elif abs(x) < 0.001:
            return f"{x:.6f}"
        elif abs(x) < 1:
            return f"{x:.4f}"
        elif abs(x) < 100:
            return f"{x:.2f}"
        else:
            return f"{x:.1f}"
    
    print(ranked_df[display_columns].head(50).to_string(
        index=False, 
        formatters={col: format_float for col in display_columns if col != 'HeadLabel'}
    ))

    # --- ç”Ÿæˆç»Ÿè®¡æ‘˜è¦ ---
    print(f"\n{'='*20} ğŸ“Š ç»Ÿè®¡æ‘˜è¦: {title} {'='*20}")
    print(f"æ³¨æ„åŠ›å æ¯”ç»Ÿè®¡:")
    print(f"  å¹³å‡å€¼: {ranked_df['AttRatio_mean'].mean():.6f}")
    print(f"  ä¸­ä½æ•°: {ranked_df['AttRatio_mean'].median():.6f}")
    print(f"  æœ€å¤§å€¼: {ranked_df['AttRatio_mean'].max():.6f} ({ranked_df.iloc[0]['HeadLabel']})")
    print(f"  æœ€å°å€¼: {ranked_df['AttRatio_mean'].min():.6f}")
    print(f"  æ ‡å‡†å·®: {ranked_df['AttRatio_mean'].std():.6f}")
    
    # è®¡ç®—ç™¾åˆ†ä½æ•°
    percentiles = [25, 50, 75, 90, 95, 99]
    print(f"\nç™¾åˆ†ä½æ•°:")
    for p in percentiles:
        value = np.percentile(ranked_df['AttRatio_mean'], p)
        print(f"  {p}%: {value:.6f}")
    
    print(f"\nå‡ºç°é¢‘ç‡ç»Ÿè®¡:")
    print(f"  æ€»å¤´æ•°: {len(ranked_df)}")
    print(f"  å¹³å‡å‡ºç°æ¬¡æ•°: {ranked_df['Frequency'].mean():.1f}")
    print(f"  æœ€å¸¸å‡ºç°å¤´: {ranked_df.loc[ranked_df['Frequency'].idxmax()]['HeadLabel']} (å‡ºç°{ranked_df['Frequency'].max()}æ¬¡)")

    # --- å¯è§†åŒ– 1: æ‰€æœ‰ Heads çš„æ³¨æ„åŠ›å æ¯”æŸ±çŠ¶å›¾ ---
    num_heads = len(ranked_df)
    fig_width = max(18, min(num_heads * 0.3, 50))  # é™åˆ¶æœ€å¤§å®½åº¦
    
    plt.figure(figsize=(fig_width, 8))
    
    # åˆ›å»ºé¢œè‰²æ˜ å°„
    if num_heads > 0:
        colors = plt.cm.Reds((ranked_df['AttRatio_mean'] - ranked_df['AttRatio_mean'].min()) / 
                             (ranked_df['AttRatio_mean'].max() - ranked_df['AttRatio_mean'].min() + 1e-8))
    else:
        colors = 'skyblue'
    
    bars = plt.bar(range(num_heads), ranked_df['AttRatio_mean'], color=colors, edgecolor='black')
    
    plt.title(f'Attention Heads by Attention Ratio in GT Region ({title})', fontsize=14, fontweight='bold')
    plt.xlabel('Attention Head (Sorted by Attention Ratio)', fontsize=12)
    plt.ylabel('Attention Ratio (GT/Total)', fontsize=12)
    
    # X è½´æ ‡ç­¾
    plt.xticks(range(num_heads), ranked_df['HeadLabel'], rotation=90, fontsize=8)
    
    # æ·»åŠ ç½‘æ ¼
    plt.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ å¹³å‡å€¼çº¿
    mean_val = ranked_df['AttRatio_mean'].mean()
    plt.axhline(y=mean_val, color='green', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.4f}')
    
    # æ ‡è®°å‰5å
    for i in range(min(5, num_heads)):
        plt.text(i, ranked_df.iloc[i]['AttRatio_mean'] + 0.001, f'#{i+1}', 
                ha='center', va='bottom', fontsize=8, fontweight='bold', color='darkred')
    
    plt.legend()
    plt.tight_layout()
    
    bar_chart_path = os.path.join(OUTPUT_DIR, f"{title}_all_heads_attention_ratio_bar.png")
    plt.savefig(bar_chart_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… æŸ±çŠ¶å›¾å·²ä¿å­˜: {bar_chart_path}")

    # --- å¯è§†åŒ– 2: Layer-Head çƒ­åŠ›å›¾ (æŒ‰æ³¨æ„åŠ›å æ¯”) ---
    try:
        pivot_table = ranked_df.pivot(index='Head', columns='Layer', values='AttRatio_mean')
        
        plt.figure(figsize=(14, 10))
        sns.heatmap(
            pivot_table, 
            cmap='Reds', 
            annot=False, 
            cbar_kws={'label': 'Attention Ratio (GT/Total)'},
            square=True
        )
        plt.title(f'Attention Head Heatmap by Attention Ratio ({title})\n(Darker Red = Higher Ratio in GT Region)', 
                  fontsize=14, fontweight='bold')
        plt.xlabel('Layer Index', fontsize=12)
        plt.ylabel('Head Index', fontsize=12)
        plt.tight_layout()
        
        heatmap_path = os.path.join(OUTPUT_DIR, f"{title}_attention_ratio_heatmap.png")
        plt.savefig(heatmap_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… çƒ­åŠ›å›¾å·²ä¿å­˜: {heatmap_path}")
    except Exception as e:
        print(f"âš ï¸  åˆ›å»ºçƒ­åŠ›å›¾å¤±è´¥: {e}")

    # --- å¯è§†åŒ– 3: æ³¨æ„åŠ›å æ¯”åˆ†å¸ƒç›´æ–¹å›¾ ---
    plt.figure(figsize=(10, 6))
    plt.hist(ranked_df['AttRatio_mean'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    plt.axvline(x=mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.4f}')
    plt.axvline(x=ranked_df['AttRatio_mean'].median(), color='green', linestyle='--', linewidth=2, 
                label=f'Median: {ranked_df["AttRatio_mean"].median():.4f}')
    
    plt.title(f'Distribution of Attention Ratio in GT Region ({title})', fontsize=14, fontweight='bold')
    plt.xlabel('Attention Ratio (GT/Total)', fontsize=12)
    plt.ylabel('Number of Heads', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    
    hist_path = os.path.join(OUTPUT_DIR, f"{title}_attention_ratio_distribution.png")
    plt.savefig(hist_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†å¸ƒç›´æ–¹å›¾å·²ä¿å­˜: {hist_path}")

    # --- å¯è§†åŒ– 4: æ³¨æ„åŠ›å æ¯” vs å±‚æ•°çš„æ•£ç‚¹å›¾ ---
    if len(ranked_df) > 1:
        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(ranked_df['Layer'], ranked_df['AttRatio_mean'], 
                             c=ranked_df['Head'], cmap='viridis', s=50, alpha=0.7)
        
        # æ·»åŠ å›å½’çº¿
        try:
            slope, intercept, r_value, p_value, std_err = stats.linregress(
                ranked_df['Layer'], ranked_df['AttRatio_mean']
            )
            x_line = np.array([ranked_df['Layer'].min(), ranked_df['Layer'].max()])
            y_line = slope * x_line + intercept
            plt.plot(x_line, y_line, 'r--', 
                    label=f'Linear fit: y={slope:.4f}x+{intercept:.4f}\nRÂ²={r_value**2:.4f}')
            
            # æ‰“å°å›å½’åˆ†æç»“æœ
            print(f"\n{'='*20} ğŸ“ˆ å›å½’åˆ†æ: æ³¨æ„åŠ›å æ¯” vs å±‚æ•° {'='*20}")
            print(f"æ–œç‡ (æ¯å±‚å˜åŒ–): {slope:.6f}")
            print(f"æˆªè·: {intercept:.6f}")
            print(f"RÂ²å€¼: {r_value**2:.4f}")
            print(f"på€¼: {p_value:.6f}")
            if p_value < 0.05:
                print("âœ… æ³¨æ„åŠ›å æ¯”ä¸å±‚æ•°çš„å…³ç³»å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ (p < 0.05)")
            else:
                print("âš ï¸  æ³¨æ„åŠ›å æ¯”ä¸å±‚æ•°çš„å…³ç³»ä¸å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§")
        except Exception as e:
            print(f"âš ï¸  å›å½’åˆ†æå¤±è´¥: {e}")
        
        plt.title(f'Attention Ratio vs Layer Index ({title})', fontsize=14, fontweight='bold')
        plt.xlabel('Layer Index', fontsize=12)
        plt.ylabel('Attention Ratio (GT/Total)', fontsize=12)
        plt.colorbar(scatter, label='Head Index')
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        
        scatter_path = os.path.join(OUTPUT_DIR, f"{title}_layer_vs_attention_ratio.png")
        plt.savefig(scatter_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… æ•£ç‚¹å›¾å·²ä¿å­˜: {scatter_path}")
    else:
        print("âš ï¸  æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºæ•£ç‚¹å›¾")

    # --- å¯è§†åŒ– 5: æ³¨æ„åŠ›å æ¯” Top 20 çš„è¯¦ç»†æ¡å½¢å›¾ ---
    top_n = min(50, len(ranked_df))
    if top_n > 0:
        plt.figure(figsize=(14, 8))
        top_data = ranked_df.head(top_n)
        
        colors = plt.cm.Reds((top_data['AttRatio_mean'] - top_data['AttRatio_mean'].min()) / 
                             (top_data['AttRatio_mean'].max() - top_data['AttRatio_mean'].min() + 1e-8))
        
        bars = plt.bar(range(top_n), top_data['AttRatio_mean'], color=colors, edgecolor='black')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (_, row) in enumerate(top_data.iterrows()):
            plt.text(i, row['AttRatio_mean'] + 0.001, 
                    f'{row["AttRatio_mean"]:.4f}\nL{row["Layer"]}H{row["Head"]}', 
                    ha='center', va='bottom', fontsize=8)
        
        plt.title(f'Top {top_n} Heads by Attention Ratio ({title})', fontsize=14, fontweight='bold')
        plt.xlabel('Head Rank', fontsize=12)
        plt.ylabel('Attention Ratio (GT/Total)', fontsize=12)
        plt.xticks(range(top_n), [f'#{i+1}' for i in range(top_n)])
        plt.grid(True, alpha=0.3, axis='y')
        plt.tight_layout()
        
        top50_path = os.path.join(OUTPUT_DIR, f"{title}_top50_attention_ratio.png")
        plt.savefig(top50_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ… Top {top_n} æ¡å½¢å›¾å·²ä¿å­˜: {top50_path}")

    # ä¿å­˜å¤šä¸ªä¸åŒnå€¼çš„å¤´åˆ°jsonlæ–‡ä»¶ï¼Œé™åˆ¶ä¸ºæŒ‡å®šå±‚èŒƒå›´
    saved_files_info = save_top_n_heads_to_jsonl(ranked_df, title, TOP_N_VALUES, LAYER_RANGE)
    
    # æ‰“å°å‰Nä¸ªå¤´çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä»¥æœ€å¤§nå€¼æ˜¾ç¤ºï¼‰
    max_n = max(TOP_N_VALUES)
    top_n_df = ranked_df.head(min(max_n, len(ranked_df)))
    
    if top_n_df is not None and not top_n_df.empty:
        print(f"\n{'='*20} ğŸ¯ å‰ {len(top_n_df)} ä¸ªæ³¨æ„åŠ›å¤´è¯¦æƒ… (æœ€å¤§n={max_n}) {'='*20}")
        for i, (_, row) in enumerate(top_n_df.iterrows()):
            if i < 20:  # åªæ˜¾ç¤ºå‰20ä¸ªè¯¦ç»†ä¿¡æ¯
                print(f"{i+1}. L{int(row['Layer'])}-H{int(row['Head'])}: æ³¨æ„åŠ›å æ¯”={row['AttRatio_mean']:.6f}")
        
        print(f"... (å…± {len(top_n_df)} ä¸ªå¤´ï¼Œè¯¦ç»†ä¿¡æ¯è§ä¿å­˜çš„æ–‡ä»¶)")
    
    return ranked_df

def main():
    """
    ä¸»å‡½æ•°ï¼šéå†ç›®å½•ï¼Œè§£ææ–‡ä»¶ï¼Œåˆ†ææ³¨æ„åŠ›å æ¯”
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    # åˆ›å»ºè¯¦ç»†çš„æ—¥å¿—æ–‡ä»¶
    log_file = os.path.join(OUTPUT_DIR, "analysis_log.txt")
    
    all_data = []
    processed_files = 0
    failed_files = 0
    
    print(f"å¼€å§‹æ‰«æç›®å½•: {ROOT_DIR}")
    print(f"æŸ¥æ‰¾æ–‡ä»¶: {TARGET_FILENAME}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"å°†ä¿å­˜ä»¥ä¸‹nå€¼çš„ç»“æœ: {TOP_N_VALUES}")
    print(f"å±‚èŒƒå›´: L{LAYER_RANGE[0]}-L{LAYER_RANGE[1]}")
    print(f"{'='*60}")
    
    with open(log_file, 'w', encoding='utf-8') as log_f:
        log_f.write(f"æ³¨æ„åŠ›å æ¯”åˆ†ææ—¥å¿—\n")
        log_f.write(f"æ‰«æç›®å½•: {ROOT_DIR}\n")
        log_f.write(f"ç›®æ ‡æ–‡ä»¶: {TARGET_FILENAME}\n")
        log_f.write(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}\n")
        log_f.write(f"å°†ä¿å­˜ä»¥ä¸‹nå€¼çš„ç»“æœ: {TOP_N_VALUES}\n")
        log_f.write(f"å±‚èŒƒå›´: L{LAYER_RANGE[0]}-L{LAYER_RANGE[1]}\n")
        log_f.write(f"{'='*60}\n\n")
    
    # éå†ç›®å½•
    for root, dirs, files in os.walk(ROOT_DIR):
        if TARGET_FILENAME in files:
            file_path = os.path.join(root, TARGET_FILENAME)
            
            print(f"\nå¤„ç†æ–‡ä»¶: {file_path}")
            
            with open(log_file, 'a', encoding='utf-8') as log_f:
                log_f.write(f"\nå¤„ç†æ–‡ä»¶: {file_path}\n")
            
            # åˆ¤æ–­æ ·æœ¬ç±»å‹ï¼ˆæ ¹æ®ç›®å½•åï¼‰
            dir_name = os.path.basename(root)
            sample_type = 'Unknown'
            if any(keyword in dir_name.lower() for keyword in ['incorrect', 'false', 'wrong', 'neg']):
                sample_type = 'Incorrect'
            elif any(keyword in dir_name.lower() for keyword in ['correct', 'true', 'right', 'pos']):
                sample_type = 'Correct'
            else:
                # å°è¯•ä»çˆ¶ç›®å½•åˆ¤æ–­
                parent_dir = os.path.basename(os.path.dirname(root))
                if any(keyword in parent_dir.lower() for keyword in ['incorrect', 'false', 'wrong', 'neg']):
                    sample_type = 'Incorrect'
                elif any(keyword in parent_dir.lower() for keyword in ['correct', 'true', 'right', 'pos']):
                    sample_type = 'Correct'
            
            print(f"  æ ·æœ¬ç±»å‹: {sample_type}")
            
            # è§£ææ•°æ®
            heads = parse_file_with_attention_ratio(file_path)
            
            if heads:
                for h in heads:
                    h['Type'] = sample_type
                    h['FilePath'] = file_path
                    h['SampleID'] = dir_name
                    all_data.append(h)
                
                processed_files += 1
                print(f"  âœ… æˆåŠŸè§£æï¼Œæ‰¾åˆ° {len(heads)} ä¸ªæ•°æ®ç‚¹")
                
                with open(log_file, 'a', encoding='utf-8') as log_f:
                    log_f.write(f"  âœ… æˆåŠŸè§£æï¼Œæ‰¾åˆ° {len(heads)} ä¸ªæ•°æ®ç‚¹\n")
            else:
                failed_files += 1
                print(f"  âš ï¸  è§£æå¤±è´¥æˆ–æ²¡æœ‰æ•°æ®")
                
                with open(log_file, 'a', encoding='utf-8') as log_f:
                    log_f.write(f"  âš ï¸  è§£æå¤±è´¥æˆ–æ²¡æœ‰æ•°æ®\n")

    # è½¬æ¢ä¸º Pandas DataFrame
    if all_data:
        full_df = pd.DataFrame(all_data)
        
        print(f"\n{'='*60}")
        print(f"å¤„ç†å®Œæˆ!")
        print(f"æˆåŠŸå¤„ç†æ–‡ä»¶: {processed_files}")
        print(f"å¤±è´¥æ–‡ä»¶: {failed_files}")
        print(f"æ€»æ•°æ®ç‚¹æ•°: {len(full_df)}")
        print(f"å°†ä¿å­˜ {len(TOP_N_VALUES)} ä¸ªä¸åŒnå€¼çš„ç»“æœæ–‡ä»¶")
        print(f"{'='*60}")
        
        with open(log_file, 'a', encoding='utf-8') as log_f:
            log_f.write(f"\n{'='*60}\n")
            log_f.write(f"å¤„ç†å®Œæˆ!\n")
            log_f.write(f"æˆåŠŸå¤„ç†æ–‡ä»¶: {processed_files}\n")
            log_f.write(f"å¤±è´¥æ–‡ä»¶: {failed_files}\n")
            log_f.write(f"æ€»æ•°æ®ç‚¹æ•°: {len(full_df)}\n")
            log_f.write(f"å°†ä¿å­˜ {len(TOP_N_VALUES)} ä¸ªä¸åŒnå€¼çš„ç»“æœæ–‡ä»¶\n")
            log_f.write(f"{'='*60}\n\n")
        
        # 1. åˆ†æå…¨éƒ¨æ ·æœ¬
        print("\n" + "="*60)
        print("å¼€å§‹åˆ†æå…¨éƒ¨æ ·æœ¬...")
        print("="*60)
        all_ranked_df = generate_report_and_plots(full_df, "All_Samples")
        
        # 2. åˆ†ææ­£ç¡®æ ·æœ¬
        correct_df = full_df[full_df['Type'] == 'Correct']
        if not correct_df.empty:
            print("\n" + "="*60)
            print(f"å¼€å§‹åˆ†ææ­£ç¡®æ ·æœ¬... (å…± {len(correct_df)} ä¸ªæ•°æ®ç‚¹)")
            print("="*60)
            correct_ranked_df = generate_report_and_plots(correct_df, "Correct_Samples")
        else:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°æ­£ç¡®æ ·æœ¬æ•°æ®")
        
        # 3. åˆ†æé”™è¯¯æ ·æœ¬
        incorrect_df = full_df[full_df['Type'] == 'Incorrect']
        if not incorrect_df.empty:
            print("\n" + "="*60)
            print(f"å¼€å§‹åˆ†æé”™è¯¯æ ·æœ¬... (å…± {len(incorrect_df)} ä¸ªæ•°æ®ç‚¹)")
            print("="*60)
            incorrect_ranked_df = generate_report_and_plots(incorrect_df, "Incorrect_Samples")
        else:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°é”™è¯¯æ ·æœ¬æ•°æ®")
        
        # 4. ä¿å­˜å®Œæ•´æ•°æ®
        full_csv_path = os.path.join(OUTPUT_DIR, "all_data_complete.csv")
        full_df.to_csv(full_csv_path, index=False)
        print(f"\nâœ… å®Œæ•´æ•°æ®å·²ä¿å­˜è‡³: {full_csv_path}")
        
        # 5. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary_path = os.path.join(OUTPUT_DIR, "analysis_summary.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("æ³¨æ„åŠ›å æ¯”åˆ†ææ±‡æ€»æŠ¥å‘Š\n")
            f.write("="*60 + "\n\n")
            
            f.write(f"æ‰«æç›®å½•: {ROOT_DIR}\n")
            f.write(f"ç›®æ ‡æ–‡ä»¶: {TARGET_FILENAME}\n")
            f.write(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}\n")
            f.write(f"å¤„ç†æ—¶é—´: {pd.Timestamp.now()}\n\n")
            
            f.write(f"å¤„ç†ç»Ÿè®¡:\n")
            f.write(f"  æˆåŠŸå¤„ç†æ–‡ä»¶æ•°: {processed_files}\n")
            f.write(f"  å¤±è´¥æ–‡ä»¶æ•°: {failed_files}\n")
            f.write(f"  æ€»æ•°æ®ç‚¹æ•°: {len(full_df)}\n")
            f.write(f"  å”¯ä¸€æ ·æœ¬æ•°: {full_df['SampleID'].nunique()}\n")
            f.write(f"  å”¯ä¸€å¤´æ•° (Layer-Headç»„åˆ): {full_df[['Layer', 'Head']].drop_duplicates().shape[0]}\n\n")
            
            f.write(f"æ ·æœ¬ç±»å‹åˆ†å¸ƒ:\n")
            type_counts = full_df['Type'].value_counts()
            for type_name, count in type_counts.items():
                f.write(f"  {type_name}: {count} ä¸ªæ•°æ®ç‚¹ ({count/len(full_df)*100:.1f}%)\n")
            
            if 'AttentionRatio' in full_df.columns:
                f.write(f"\næ³¨æ„åŠ›å æ¯”æ€»ä½“ç»Ÿè®¡:\n")
                f.write(f"  å¹³å‡å€¼: {full_df['AttentionRatio'].mean():.6f}\n")
                f.write(f"  ä¸­ä½æ•°: {full_df['AttentionRatio'].median():.6f}\n")
                f.write(f"  æ ‡å‡†å·®: {full_df['AttentionRatio'].std():.6f}\n")
                f.write(f"  æœ€å°å€¼: {full_df['AttentionRatio'].min():.6f}\n")
                f.write(f"  æœ€å¤§å€¼: {full_df['AttentionRatio'].max():.6f}\n")
                
                f.write(f"\nå±‚èŒƒå›´ç»Ÿè®¡ (L{LAYER_RANGE[0]}-L{LAYER_RANGE[1]}):\n")
                layer_filtered = full_df[(full_df['Layer'] >= LAYER_RANGE[0]) & (full_df['Layer'] <= LAYER_RANGE[1])]
                if not layer_filtered.empty:
                    f.write(f"  æ•°æ®ç‚¹æ•°: {len(layer_filtered)}\n")
                    f.write(f"  æ³¨æ„åŠ›å æ¯”å¹³å‡å€¼: {layer_filtered['AttentionRatio'].mean():.6f}\n")
                else:
                    f.write(f"  åœ¨æŒ‡å®šå±‚èŒƒå›´å†…æ²¡æœ‰æ•°æ®\n")
            
            # æ‰¾å‡ºæœ€å¸¸å‡ºç°çš„å¤´
            f.write(f"\næœ€å¸¸å‡ºç°çš„å‰10ä¸ªå¤´ (è·¨æ‰€æœ‰æ ·æœ¬):\n")
            head_counts = full_df.groupby(['Layer', 'Head']).size().reset_index(name='Count')
            head_counts = head_counts.sort_values('Count', ascending=False).head(10)
            for i, (_, row) in enumerate(head_counts.iterrows()):
                f.write(f"  {i+1}. L{int(row['Layer'])}-H{int(row['Head'])}: {int(row['Count'])} æ¬¡\n")
            
            # è®°å½•ä¿å­˜çš„æ–‡ä»¶æ•°é‡
            f.write(f"\nä¿å­˜çš„æ–‡ä»¶ç»Ÿè®¡:\n")
            f.write(f"  ä¿å­˜äº† {len(TOP_N_VALUES)} ä¸ªä¸åŒnå€¼çš„ç»“æœæ–‡ä»¶\n")
            f.write(f"  nå€¼åˆ—è¡¨: {TOP_N_VALUES}\n")
            f.write(f"  æ¯ç§ç±»å‹æ ·æœ¬ä¿å­˜ {len(TOP_N_VALUES) * 2} ä¸ªæ–‡ä»¶ (JSONL + CSV)\n")
            if 'Type' in full_df.columns:
                type_list = full_df['Type'].unique()
                f.write(f"  æ¶‰åŠ {len(type_list)} ç§æ ·æœ¬ç±»å‹\n")
        
        print(f"âœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜è‡³: {summary_path}")
        
    else:
        print("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œæ–‡ä»¶å†…å®¹ã€‚")
        with open(log_file, 'a', encoding='utf-8') as log_f:
            log_f.write("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œæ–‡ä»¶å†…å®¹ã€‚\n")
    
    print(f"\nâœ… åˆ†æå®Œæˆ! æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")
    print(f"ğŸ“ è¯¦ç»†æ—¥å¿—: {log_file}")
    print(f"ğŸ“Š å…±ä¿å­˜äº† {len(TOP_N_VALUES)} ä¸ªä¸åŒnå€¼çš„ç»“æœæ–‡ä»¶")

if __name__ == "__main__":
    main()